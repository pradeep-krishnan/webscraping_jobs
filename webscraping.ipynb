{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scraping indeed website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from time import sleep\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Beautiful Soup to scrape indeed job postings, descripion on clicks, reviews from company's review page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 1000\n",
    "i = 0\n",
    "results = []\n",
    "df1 = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Summary\", \"Description\", \"Review\"])\n",
    "for city in set(['Atlanta',\n",
    " 'Austin',\n",
    " 'Baltimore',\n",
    " 'Boston',\n",
    " 'Charlotte',\n",
    " 'Chicago',\n",
    " 'Cincinnati',\n",
    " 'Columbus',\n",
    " 'Dallas',\n",
    " 'Denver',\n",
    " 'Houston',\n",
    " 'Indianapolis',\n",
    " 'Jacksonville',\n",
    " 'Kansas+City',\n",
    " 'Los+Angeles',\n",
    " 'New+York',\n",
    " 'Miami',\n",
    " 'Minneapolis',\n",
    " 'Nashville',\n",
    " 'Oakland',\n",
    " 'Philadelphia',\n",
    " 'Phoenix',\n",
    " 'Pittsburgh',\n",
    " 'Portland',\n",
    " 'San+Diego',\n",
    " 'San+Francisco',\n",
    " 'San+Jose',\n",
    " 'Seattle',\n",
    " 'St.+Louis',\n",
    " 'Tomp',\n",
    " 'Washington%2C+DC']):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        url = link.format(city, start)\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        for each in soup.find_all(class_= \"result\" ):\n",
    "            try: \n",
    "                title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "            except:\n",
    "                title = None\n",
    "            try:\n",
    "                location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "            except:\n",
    "                location = None\n",
    "            try: \n",
    "                company = each.find(class_='company').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = None\n",
    "            try:\n",
    "                salary = each.find('span', {'class':'no-wrap'}).text\n",
    "            except:\n",
    "                salary = None\n",
    "            try:\n",
    "                summary = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "            except:\n",
    "                summary = None\n",
    "            try: \n",
    "                description_link = (\"%s%s\" % (\"https://www.indeed.com\",each.find('a').get('href')))\n",
    "            except:\n",
    "                description_link = 'None'\n",
    "            try:\n",
    "                review_link = (\"%s%s\" % (\"https://www.indeed.com\",each.find('div').find('a').get('href')))            \n",
    "            except: \n",
    "                review_link = 'None'\n",
    "            df1 = df1.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Summary':summary,\\\n",
    "                                      'Description':description_link, 'job_description': None, 'Review': review_link,\\\n",
    "                           'comp_rating_overall': None, 'wl_bal_rating': None, 'benefit_rating': None, 'jsecurity_rating': None,\\\n",
    "                            'mgmt_rating': None, 'culture_rating': None}, ignore_index=True)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('jobs_desc_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jobdesc(df):\n",
    "    for i in range(0,len(df)):\n",
    "        company_name = df.iloc[i]['Company']\n",
    "        desc = df.iloc[i]['Description']\n",
    "        html = requests.get(desc)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        try:\n",
    "            df.loc[df['Company'] == company_name, 'job_description'] = soup.find(class_=\"jobsearch-JobComponent-description icl-u-xs-mt--md\").text.replace('\\n', ' ')\n",
    "        except:\n",
    "            df.loc[df['Company'] == company_name, 'job_description'] = 'None'\n",
    "        sleep(0.10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jobdesc_again(df):\n",
    "    for i in range(0,len(df)):\n",
    "        if(df.iloc[i]['job_description'] == 'None'):\n",
    "            company_name = df.iloc[i]['Company']\n",
    "            desc = df.iloc[i]['Description']\n",
    "            html = requests.get(desc)\n",
    "            soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'job_description'] = soup.find(class_=\"jobsearch-JobComponent-description icl-u-xs-mt--md\").text.replace('\\n', ' ')\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'job_description'] = 'None'\n",
    "            sleep(0.10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviews(df):\n",
    "    for i in range(0,len(df)):\n",
    "        if(df.iloc[i]['Review'] != 'None'):\n",
    "            company_name = df.iloc[i]['Company']\n",
    "            cmp_page = df.iloc[i]['Review']\n",
    "            html = requests.get(cmp_page)\n",
    "            soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'comp_rating_overall'] = float(soup.find(class_=\"cmp-header-rating-average\").text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'comp_rating_overall'] = 'None'\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'wl_bal_rating'] = float(soup.find(\"div\",{\"class\": \"cmp-ReviewCategory cmp-ReviewCategory--textmd\"}).\\\n",
    "                        find_all(\"span\", {\"class\": \"cmp-ReviewCategory-rating\"})[0].text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'wl_bal_rating'] = 'None'\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'benefit_rating'] = float(soup.find(\"div\",{\"class\": \"cmp-ReviewCategory cmp-ReviewCategory--textmd\"}).\\\n",
    "                        find_all(\"span\", {\"class\": \"cmp-ReviewCategory-rating\"})[1].text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'benefit_rating'] = 'None'\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'jsecurity_rating'] = float(soup.find(\"div\",{\"class\": \"cmp-ReviewCategory cmp-ReviewCategory--textmd\"}).\\\n",
    "                        find_all(\"span\", {\"class\": \"cmp-ReviewCategory-rating\"})[2].text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'jsecurity_rating'] = 'None'\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'mgmt_rating'] = float(soup.find(\"div\",{\"class\": \"cmp-ReviewCategory cmp-ReviewCategory--textmd\"}).\\\n",
    "                        find_all(\"span\", {\"class\": \"cmp-ReviewCategory-rating\"})[3].text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'mgmt_rating'] = 'None'\n",
    "            try:\n",
    "                df.loc[df['Company'] == company_name, 'culture_rating'] = float(soup.find(\"div\",{\"class\": \"cmp-ReviewCategory cmp-ReviewCategory--textmd\"}).\\\n",
    "                        find_all(\"span\", {\"class\": \"cmp-ReviewCategory-rating\"})[4].text.replace('\\n', ' '))\n",
    "            except:\n",
    "                df.loc[df['Company'] == company_name, 'culture_rating'] = 'None'\n",
    "            sleep(0.10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('jobs_desc_ratings.csv', index = False)\n",
    "df =  pd.read_csv('jobs_desc_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = get_jobdesc(df)\n",
    "df = get_jobdesc_again(df)\n",
    "df = get_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('jobs_complete.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_city_state(df,col):\n",
    "    df = df.join(df[col].str.split(',', 1, expand=True).rename(columns={0:'City', 1:'State'}))\n",
    "    df['City'] = df['City'].astype(str)\n",
    "    df['State'] = df['State'].astype(str)\n",
    "    df['State'] = [x[0:3] for x in df['State'] if x != None]\n",
    "    df.drop(['Location'], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = get_city_state(df, 'Location')\n",
    "df = df[['City', 'State']]\n",
    "new_df = pd.DataFrame(df4.groupby('City')['alternate'].apply(set)).reset_index()\n",
    "new_df.columns = ['City', 'center']\n",
    "new_df['centre'] = new_df['center'].apply(lambda x: list(x))\n",
    "new_df['centre1'] = [x[0] for x in new_df['centre']]\n",
    "new_df = new_df[['City', 'centre1']]\n",
    "new_df = new_df.replace('Washington%2C DC', 'Washington DC')\n",
    "new_df.to_csv('city_list.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_df = pd.DataFrame(df4['City'].value_counts().index)\n",
    "city_df.columns = ['cities']\n",
    "city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_df['new_city'] = df4.loc[(df4['City'] == city_df['cities']), 'alternate']\n",
    "city_df.to_csv('city_list.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
